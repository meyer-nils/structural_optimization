\chapter{Introduction}
The first chapter introduces the basic terms for structural optimization. In addition, it introduces the mathematical notation used throughout this lecture and repeats some basic concepts of tensor algebra and tensor analysis. 

\begin{objectives}{}{objectives_introduction}
After studying this chapter and finishing the exercise, you should be able to 
\begin{itemize}[label=$\dots$]
    \item distinguish three types of structural optimization problems.
    \item perform tensor algebra computations such as inner products, outer products, and tensor contractions.
    \item apply differential operators such as divergence and gradient on tensor fields.
\end{itemize}
\end{objectives}


\section{What is structural optimization?}
Following Gordon \cite{Gordon2003}, a \emph{structure} is "any assemblage of materials which is intended to sustain loads". The term \emph{optimization} means "making things best". Thus, \emph{structural optimization} is the subject of making an assemblage of materials sustain loads in the best way \cite{Christensen2008}. 

The term \emph{best} depends on what we define to be the \emph{objective function} of our design problem. It could be the weight to obtain light structures, it could be the cost to obtain cheap structures, or it could be CO\textsubscript{2} emissions to obtain sustainable structures. Usually the optimization is subject to some \emph{constraints}, for example a certain load that has to be endured without permanent deformation. The variables that we can change to optimize the objective function are called \emph{design variables}. Depending on the choice of design variables, we generally divide three types of structural optimization problems: 
\begin{description}
    \item[Sizing optimization]{The design variables describe some technical parameters of a structure, e.g. wall thickness, diameters of holes, or the cross section of beams. We seek the optimal size of these parameters in he optimization task.}
    \item[Shape optimization]{The design variables describe the contour or boundary of a part. We seek the optimal shape of the part boundaries in the optimization task.}
    \item[Topology optimization]{The design variables describe the presence of material in a design space. We seek the optimal distribution of material within that design space.}
\end{description}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=\textwidth, alt={Sketch topology optimization, shape optimization, sizing optimization}]{figures/structural_optimization_types.png}
    \caption{Types of structural optimization (left to right): Topology optimization, shape optimization, sizing optimization.}
    \label{fig:structural_optimization_types.}
\end{figure}

Before we can start optimizing structures, we need to recapitulate some basic tensor math, continuum mechanics and fundamentals of optimization, though. 

\section{Tensor algebra}
During this lecture we will deal with spaces, scalars, vectors, tensors, matrices and other mathematical objects. To support understanding of which objects are used in each context, we introduce some notation rules. 

\subsection{Scalars}
A scalar variable, such as a temperature, has no direction and is completely described by a single real value. Scalars are denoted as variables in regular font, for example
\begin{equation}
    a \in \mathcal{R}.
\end{equation}

\subsection{Vectors}
Most objects have some sort of direction associated to it (e.g. a velocity or a force). They are defined in a \emph{vector space} 
$\mathcal{R}^d$, where $d$ is the dimension of the vector space. A \emph{vector} is an element of the vector space that fulfills so-called vector axioms and is denoted in bold font, e.g. 
\begin{equation}
    \textbf{a} \in \mathcal{R}^d.
\end{equation}
In order to describe components of a vector, it is convenient to define a \emph{basis}. A basis is given by a subset of vectors ${\mathbf{e}_1,\mathbf{e}_2,...,\mathbf{e}_d}$ that are linearly independent and span the entire vector space.
More specifically, we use an orthonormal basis (\emph{ONB}) that is spanned by normalized and orthogonal vectors $\{\mathbf{e}_i\}$. The orthogonality is expressed as  
\begin{equation}
    \label{eq:orthogonality}
    \mathbf{e}_i \cdot \mathbf{e}_j = \delta_{ij}
\end{equation}
with the \emph{Kronecker symbol} 
\begin{equation}
    \delta_{ij} = 
    \begin{cases}
    1 & \text{if }  i=j \\
    0 & \text{else}.
    \end{cases}
\end{equation}

\begin{example}{Standard basis in 3D Euclidean space}{basis_example}
    The standard basis for the three-dimensional ($d=3$) Euclidean space is given by the set of three vectors
    \begin{equation}
        \{\mathbf{e}_1=(1,0,0), \mathbf{e}_2=(0,1,0), \mathbf{e}_3=(0,0,1)\}.
    \end{equation}
\end{example}

With a basis, we can define a vector in terms of \emph{coordinates} $(a_1, a_2, ..., a_d)$ as 
\begin{equation}
    \label{eq:coordinates}
    \mathbf{a} = \sum_i a_i \mathbf{e}_i. 
\end{equation}
It is important to note that these coordinates depend on the basis. The same vector $\mathbf{a}$ may be expressed with coordinates $(a_1, a_2, ..., a_d)$ for a basis $\{\mathbf{e}_i\}$ and $(\tilde{a}_1, \tilde{a}_2, ..., \tilde{a}_d)$ for a basis $\{\tilde{\mathbf{e}}_i\}$. 

We define the \emph{inner product} or \emph{scalar product} of two vectors $\mathbf{a}, \mathbf{b} \in \mathcal{R}^d$ as 
\begin{equation}
    \mathbf{a} \cdot \mathbf{b} 
        = \sum_i a_i \mathbf{e}_i \cdot \sum_j b_j \mathbf{e}_j 
        =  \sum_{i,j} a_i b_j \overbracket{(\mathbf{e}_i \cdot \mathbf{e}_j)}^{\delta_{ij}} 
        = \sum_{i,j} a_i b_j \delta_{ij} = \sum_{i} a_i b_i
\end{equation}
using Equation \eqref{eq:orthogonality}.

There are a couple of remarks at this point:
\begin{itemize}
    \item The result of the scalar product between two vectors is a scalar.
    \item The Kronecker symbol can be interpreted as an operator that replaces an index.
    \item A single dot always indicates a single contraction of the inner dimensions.
    \item Some authors identify vectors with column vectors and write the dot product as a matrix product $\mathbf{a}^\top \mathbf{b}$, where $\mathbf{a}^\top$ denotes the transpose of $\mathbf{a}$.
\end{itemize}

\subsection{Tensors}
Vectors are not sufficient to describe all directional objects needed in this lecture. A stress, for example, results in a traction vector that also depends on the cutting plane defined by its normal direction. 
In a sense, the stress tensor combines two directions and we learn in basic mechanics courses that one index is attributed to the direction and one to the plane. We can also think about the stress tensor as a mapping from one vector to another vector or as a multi-dimensional array containing the values of $\sigma_{ij}$.

Such \emph{tensor} objects can be constructed by the \emph{outer product}, \emph{tensor product}, or \emph{dyadic product} of two vectors $\mathbf{a}, \mathbf{b} \in \mathcal{R}^d$ as 
\begin{equation}
    \label{eq:tensorproduct}
    \mathbf{a} \otimes \mathbf{b} = \sum_{i,j} a_i b_j \mathbf{e}_i \otimes \mathbf{e}_j,
\end{equation}
where the term $\mathbf{e}_i \otimes \mathbf{e}_j$ may be interpreted as a new basis for the tensor. We may also specify such a tensor $\mathbf{A} \in \mathcal{R}^{d \times d}$ directly as 
\begin{equation}
    \mathbf{A} = \sum_{i,j} A_{ij} \mathbf{e}_i \otimes \mathbf{e}_j ,
\end{equation}
where bold variables are used for such tensors.
The entries of these tensors may be interpreted as two-dimensional arrays
\begin{equation}
    \mathbf{a} \otimes \mathbf{b} =
    \begin{pmatrix}
         a_1 b_1    & a_1 b_2   & \dots   & a_1 b_d  \\
         a_2 b_1    & a_2 b_2   & \dots   & a_2 b_d  \\
         \dots      & \dots     & \dots   & \dots  \\
         a_d b_1    & a_d b_2   & \dots   & a_d b_d  \\
    \end{pmatrix}_{\{\mathbf{e}_i\}}
\end{equation}
and 
\begin{equation}
    \mathbf{A} =
    \begin{pmatrix}
         A_{11}    & A_{12}   & \dots   & A_{1d}  \\
         A_{21}    & A_{22}   & \dots   & A_{2d}  \\
         \dots      & \dots     & \dots   & \dots  \\
         A_{d1}    & A_{d2}   & \dots   & A_{dd}  \\
    \end{pmatrix}_{\{\mathbf{e}_i\}}
\end{equation}
for this specific ONB.

In fact, we can interpret some objects of the previous chapter as tensors. A vector is a \emph{first-order} tensor and the Kronecker symbol can be used to define a \emph{second-order} unit tensor 
\begin{equation}
    \pmb{I} = \sum_{i,j} \delta_{ij} \mathbf{e}_i \otimes \mathbf{e}_j = 
    \begin{pmatrix}
         1      & 0     & \dots & 0 \\
         0      & 1     & \dots & 0 \\
         \dots  & \dots & \dots & \dots  \\
         0      & 0     & \dots & 1  \\
    \end{pmatrix}_{\{\mathbf{e}_i\}}.
\end{equation}

In addition, we define the \emph{trace}, \emph{transpose}, and \emph{inverse} of a second-order tensor $\mathbf{A} \in \mathcal{R}^{d \times d}$ as 
\begin{align}
    \textrm{tr}(\mathbf{A}) &= \sum_{i} A_{ii}, \\
    \mathbf{A}^\top &= \sum_{i,j} A_{ji} \mathbf{e}_i \otimes \mathbf{e}_j, \textrm{and}\\
    \pmb{I} &= \mathbf{A} \cdot \mathbf{A}^{-1} ,
\end{align}
respectively.

We define the tensor contraction of two second-order tensors $\mathbf{A},\mathbf{B} \in \mathcal{R}^{d \times d}$: 
\begin{align}
    \mathbf{A} \cdot \mathbf{B} 
        &= \sum_{i,j} A_{ij} \mathbf{e}_i \otimes \mathbf{e}_j \cdot \sum_{k,l}B_{kl} \mathbf{e}_k \otimes \mathbf{e}_l \\
        &= \sum_{i,j,k,l} A_{ij} B_{kl} (\mathbf{e}_i \otimes \overbracket{\mathbf{e}_j) \cdot  (\mathbf{e}_k}^{\delta_{jk}} \otimes \mathbf{e}_l) \\
        &= \sum_{i,j,k,l} A_{ij} B_{kl} (\overbracket{\mathbf{e}_j \cdot  \mathbf{e}_k}^{\delta_{jk}}) \mathbf{e}_i \otimes \mathbf{e}_l \\
        &= \sum_{i,j,k,l} A_{ij}B_{kl} \delta_{jk} \mathbf{e}_i \otimes \mathbf{e}_l \\
        &= \sum_{i,j,l} A_{ij}B_{jl} \mathbf{e}_i \otimes \mathbf{e}_l
\end{align}
Essentially, this contracts the inner dimensions once. These are some examples of such contractions:

\begin{example}{Tensor contraction}{tensorcontractionexample}
    Contraction of two first order tensors $\mathbf{a},\mathbf{b} \in \mathcal{R}^d$: 
    \begin{equation}
        \mathbf{a} \cdot \mathbf{b} = \sum_i a_i b_i \quad \in \mathcal{R}
    \end{equation}
    Contraction of two second-order tensors $\mathbf{A},\mathbf{B} \in \mathcal{R}^{d \times d}$: 
    \begin{equation}
        \mathbf{A} \cdot \mathbf{B} = \sum_{i,j,k} A_{ij} B_{jk} \mathbf{e}_i \otimes \mathbf{e}_k \quad \in \mathcal{R}^{d \times d}
    \end{equation}
    Contraction of a second-order tensor $\mathbf{A} \in \mathcal{R}^{d \times d}$ and a first-order tensor $\mathbf{b} \in \mathcal{R}^d$: 
    \begin{equation}
        \mathbf{A} \cdot \mathbf{b} = \sum_{i,j} A_{ij} b_j \mathbf{e}_i \quad \in \mathcal{R}^d
    \end{equation}
\end{example}

We may extend the concept of outer products further to higher orders. A stiffness tensor, for example, maps a second-order strain tensor to a second-order stress tensor and thus it is a fourth-order tensor, which we denote with double-stroke capital letters. More general, a fourth order tensor may be obtained by an outer product of two second-order tensors $\mathbf{A},\mathbf{B}\in \mathcal{R}^{d \times d}$ as 
\begin{equation}
    \mathbb{C} = \mathbf{A} \otimes \mathbf{B} = \sum_{i,j,k,l} A_{ij}B_{kl}  \mathbf{e}_i \otimes \mathbf{e}_j \otimes \mathbf{e}_k \otimes \mathbf{e}_l.
\end{equation}

We can apply the inner product multiple times. For example, we can define the double contraction of two second-order tensors $\mathbf{A},\mathbf{B} \in \mathcal{R}^{d \times d}$: 
\begin{align}
    \mathbf{A} : \mathbf{B} 
        &= \sum_{i,j} A_{ij} \mathbf{e}_i \otimes \mathbf{e}_j :  \sum_{k,l} B_{kl} \mathbf{e}_k \otimes \mathbf{e}_l \\
        &= \sum_{i,j,k,l} A_{ij}B_{kl} (\mathbf{e}_i \otimes \mathbf{e}_j) :  (\mathbf{e}_k \otimes \mathbf{e}_l) \\
        &= \sum_{i,j,k,l} A_{ij}B_{kl} \delta_{ik} \delta_{jl} \\
        &= \sum_{i,j} A_{ij}B_{ij}
\end{align}

\begin{example}{Tensor double contraction}{tensordoublecontractionexample}
    Double contraction of two second-order tensors $\mathbf{A},\mathbf{B} \in \mathcal{R}^{d \times d}$: 
    \begin{equation}
        \mathbf{A} : \mathbf{B} = \sum_{i,j} A_{ij} B_{ij} \quad \in \mathcal{R}
    \end{equation}
    Double contraction of two fourth-order tensors $\mathbb{A},\mathbb{B} \in \mathcal{R}^{d \times d \times d \times d}$: 
    \begin{equation}
        \mathbb{A} : \mathbb{B} = \sum_{i,j,k,l,m,n} A_{ijkl} B_{klmn} \mathbf{e}_i \otimes \mathbf{e}_j \otimes \mathbf{e}_m \otimes \mathbf{e}_n \quad \in \mathcal{R}^{d \times d \times d \times d}
    \end{equation}
    Double contraction of a fourth-order tensor $\mathbb{A} \in \mathcal{R}^{d \times d \times d \times d}$ and a second-order tensor $\mathbf{B} \in \mathcal{R}^{d \times d}$: 
    \begin{equation}
        \mathbb{A} : \mathbf{B} = \sum_{i,j,k,l} A_{ijkl} B_{kl} \mathbf{e}_i \otimes \mathbf{e}_j \quad \in \mathcal{R}^{d \times d}
    \end{equation}
\end{example}

There are a couple of remarks at this point:
\begin{itemize}
    \item The number of dots in the product indicates how many inner dimensions we want to contract.
    \item The components or matrix entries of the same tensor can be different in a different basis.
    \item If vectors are identified with column vectors, the outer product ist written as a matrix product $\mathbf{a} \otimes \mathbf{b} = \mathbf{a} \mathbf{b}^\top$.
\end{itemize}

\section{Tensor analysis}
In many physical processes, variables appear as \emph{tensor fields} with a dependency on a spatial variable $\mathbf{x} \in \mathcal{R}^d$. Some examples are given here:
\begin{example}{Tensor fields}{tensorfieldexample}
    A temperature field is a scalar field
    \begin{equation}
        \theta: \mathcal{R}^d \rightarrow \mathcal{R}.
    \end{equation}
    A displacement field is a vector field
    \begin{equation}
        \mathbf{u}: \mathcal{R}^d \rightarrow \mathcal{R}^d.
    \end{equation}
    A stress field is a second-order tensor field
    \begin{equation}
        \pmb{\sigma}: \mathcal{R}^d \rightarrow \mathcal{R}^{d \times d}.
    \end{equation}
\end{example}

In most governing equations of physical processes, we need to compute differential operations on these fields. Therefore, the following sections introduce the most important differential operators for tensor fields assuming that the fields are differentiable and that we are using Cartesian coordinates. 
% \footnote{In curvilinear coordinates things get a bit more complicated. We stick to Cartesian coordinated in this lecture. }. 

A common notation for differential operations on tensor fields utilizes the Nabla operator 
\begin{equation}
    \nabla = \sum_i \frac{\partial }{\partial x_i} \mathbf{e}_i = 
    \begin{pmatrix}
            \frac{\partial}{\partial x_1} \\
            \frac{\partial}{\partial x_2} \\
            \dots \\
            \frac{\partial}{\partial x_d} \\
        \end{pmatrix}_{\{\mathbf{e}_i\}}
\end{equation}
The Nabla operator may be interpreted as a first-order vector, which applies a derivative. 

\subsection{Gradient}
The gradient computes the partial derivative with respect to each spatial direction and represents this information as a higher-order tensor. We denote the gradient operator $\nabla (\bullet)$. The resulting tensor is increased by one order as shown in the following examples:
\begin{example}{Gradient}{gradientsexample}
    Gradient of a scalar field
    \begin{equation}
        \nabla \theta = \sum_i \frac{\partial \theta}{\partial x_i} \mathbf{e}_i =
        \begin{pmatrix}
            \frac{\partial \theta}{\partial x_1} \\
            \frac{\partial \theta}{\partial x_2} \\
            \dots \\
            \frac{\partial \theta}{\partial x_d} \\
        \end{pmatrix}_{\{\mathbf{e}_i\}}
    \end{equation}
    Gradient of a vector field
    \begin{equation}
        \nabla \mathbf{u} = \sum_{i,j} \frac{\partial u_i}{\partial x_j} \mathbf{e}_i \otimes \mathbf{e}_j
        = 
        \begin{pmatrix}
            \frac{\partial u_1}{\partial x_1} & \frac{\partial u_1}{\partial x_2} & \dots & \frac{\partial u_1}{\partial x_d}\\
            \frac{\partial u_2}{\partial x_1} & \frac{\partial u_2}{\partial x_2} & \dots & \frac{\partial u_2}{\partial x_d}\\
            \dots & \dots & \dots & \dots\\
            \frac{\partial u_d}{\partial x_1} & \frac{\partial u_d}{\partial x_2} & \dots & \frac{\partial u_d}{\partial x_d}\\
        \end{pmatrix}_{\{\mathbf{e}_i\}}
    \end{equation}
\end{example}

The gradient may be interpreted as a measure for how much a tensor changes in each direction. 

We may compute the gradient of a gradient of a scalar field. If all the partial derivatives exits, we call the entries a \emph{Hessian}.

\begin{example}{Hessian}{hessianexample}
    Hessian of a scalar field
    \begin{equation}
        \nabla^2 \theta = \sum_{i,j} \frac{\partial^2 \theta}{\partial x_i \partial x_j} \mathbf{e}_i \otimes \mathbf{e}_j
        = 
        \begin{pmatrix}
            \frac{\partial^2 \theta}{\partial x_1 \partial x_1} & \frac{\partial^2 \theta}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2 \theta}{\partial x_1 \partial x_d}\\
            \frac{\partial^2 \theta}{\partial x_2 \partial x_1} & \frac{\partial^2 \theta}{\partial x_2 \partial x_2} & \dots & \frac{\partial^2 \theta}{\partial x_2 \partial x_d}\\
            \dots & \dots & \dots & \dots\\
            \frac{\partial^2 \theta}{\partial x_d \partial x_1} & \frac{\partial^2 \theta}{\partial x_d \partial x_2} & \dots & \frac{\partial^2 \theta}{\partial x_d \partial x_d}\\
        \end{pmatrix}_{\{\mathbf{e}_i\}}
    \end{equation}
\end{example}

\subsection{Divergence}
The divergence computes the partial derivative with respect to each spatial direction, sums these values up and represents this information as a lower-order tensor. We denote the divergence operator $\nabla \cdot (\bullet)$. The resulting tensor is increased by one order as shown in the following examples:
\begin{example}{Divergence}{divergenceexample}
    Divergence of a vector field
    \begin{equation}
        \nabla \cdot \mathbf{u} =  \sum_i \frac{\partial u_i}{\partial x_i}
        = 
        \frac{\partial u_1}{\partial x_1} + \frac{\partial u_2}{\partial x_2} + \dots + \frac{\partial u_d}{\partial x_d}
    \end{equation}
    Divergence of a second-order tensor field
    \begin{equation}
        \nabla \cdot \pmb{\sigma} = \sum_{i,j} \frac{\partial \sigma_{ij}}{\partial x_j} \mathbf{e}_i
        = 
        \begin{pmatrix}
            \frac{\partial \sigma_{11}}{\partial x_1} + \frac{\partial \sigma_{12}}{\partial x_2} + \frac{\partial \sigma_{13}}{\partial x_3} \\
            \frac{\partial \sigma_{21}}{\partial x_1} + \frac{\partial \sigma_{22}}{\partial x_2} + \frac{\partial \sigma_{23}}{\partial x_3}\\
            \frac{\partial \sigma_{31}}{\partial x_1} + \frac{\partial \sigma_{32}}{\partial x_2} + \frac{\partial \sigma_{33}}{\partial x_3}
        \end{pmatrix}_{\{\mathbf{e}_i\}}
    \end{equation}
\end{example}

The divergence may be interpreted as the tensor field's source. If the divergence is positive, this is a source and if it is negative, it will act as a sink. 

\bibliographystyle{unsrtnat}
\bibliography{literature} 

% SEE https://biomechanics.stanford.edu/me337/me337_s02.pdf