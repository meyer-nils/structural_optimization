{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02 Gradient decent methods\n",
    "\n",
    "\n",
    "We re-use the quadratic function from last exercise $f: \\mathcal{R}^2 \\rightarrow \\mathcal{R}$ defined as \n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = (\\mathbf{x} - \\tilde{\\mathbf{x}}) \\cdot \\mathbf{Q} \\cdot (\\mathbf{x} - \\tilde{\\mathbf{x}})\n",
    "$$\n",
    "with \n",
    "$$\n",
    "\\mathbf{Q} = \n",
    "\\begin{pmatrix}\n",
    "    2 & 1 \\\\\n",
    "    1 & 1 \n",
    "\\end{pmatrix} \n",
    "\\quad \n",
    "\\text{and}\n",
    "\\quad\n",
    "\\tilde{\\mathbf{x}} = \n",
    "\\begin{pmatrix}\n",
    "    -1\\\\\n",
    "    1 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "to test the implemented gradient decent methods. \n",
    "The solution to the problem \n",
    "$$\n",
    "\\min_{\\mathbf{x}} f(\\mathbf{x})\n",
    "$$\n",
    "is $\\mathbf{x}^*=\\tilde{\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from utils import plot_contours\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# Define domain\n",
    "x0 = torch.linspace(-5, 5, steps=100)\n",
    "x1 = torch.linspace(-5, 5, steps=100)\n",
    "x = torch.stack(torch.meshgrid(x0, x1, indexing=\"xy\"), dim=2)\n",
    "\n",
    "# Define constants\n",
    "xt = torch.tensor([-1.0, 1.0])\n",
    "Q = torch.tensor([[2.0, 1.0], [1, 1.0]])\n",
    "\n",
    "\n",
    "# Define function\n",
    "def f(x):\n",
    "    dx = x - xt\n",
    "    return torch.einsum(\"...i,ij,...j\", dx, Q, dx)\n",
    "\n",
    "\n",
    "# Plot function as contour lines\n",
    "plot_contours(x[..., 0], x[..., 1], f(x), opti=[-1, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Simple steepest decent\n",
    "\n",
    "We have a predefined function named `simple_decent(x_init, func, eta=0.1, maxiter=100)` that takes an initial point $\\mathbf{x}_0 \\in \\mathcal{R}^d$ named `x_init`, a function `func`, a step size `eta`, and an iteration limit `max_iter`. \n",
    "\n",
    "a) Implement a simple steepest gradient decent in that function. The function should return a list of all steps $\\mathbf{x}_k \\in \\mathcal{R}^d$ taken during the optimization, i.e. `[[x1_0, x2_0, ..., xd_0], [x1_1, x2_1, ..., xd_1], ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_decent(x_init, func, eta=0.1, max_iter=100):\n",
    "    # Copy initial x to new differentiable tensor x\n",
    "    x = x_init.clone().requires_grad_()\n",
    "\n",
    "    points = []\n",
    "\n",
    "    # --> Implement your solution here\n",
    "\n",
    "    return points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test the function with the following code for \n",
    "$$\n",
    "    \\mathbf{x}_0 = \n",
    "\\begin{pmatrix}\n",
    "    4\\\\\n",
    "    -1 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and play around with the optional parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = torch.tensor([4.0, -1.0])\n",
    "path = simple_decent(x_init, f)\n",
    "plot_contours(\n",
    "    x[..., 0], x[..., 1], f(x), opti=[-1, 1], paths={\"Simple steepest decent\": path}\n",
    ")\n",
    "print(f\"Final values are x_1={path[-1][0]:.3f}, x_2={path[-1][1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Steepest decent method with incomplete line search\n",
    "\n",
    "We have a predefined function named `incomplete_line_search(x_init, func, eta_0=5.0, c=0.5, rho=0.8, maxiter=10)` that takes an initial point $\\mathbf{x}_0 \\in \\mathcal{R}^d$ named `x_init`, a function `func`, an initial step size `eta_0`, a Armijo constant `c`, a backtracking reduction factor `rho` and an iteration limit `max_iter`.\n",
    "\n",
    "a) Implement a steepest gradient decent with incompleted line search using the backtracking algorithm in that function. The function should return a list of all steps $\\mathbf{x}_k \\in \\mathcal{R}^d$ taken during the optimization, i.e. `[[x1_0, x2_0, ..., xd_0], [x1_1, x2_1, ..., xd_1], ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incomplete_line_search(x_init, func, eta_0=5.0, c=0.5, rho=0.8, max_iter=10):\n",
    "    # Copy initial x to new differentiable tensor x\n",
    "    x = x_init.clone().requires_grad_()\n",
    "\n",
    "    points = []\n",
    "\n",
    "    # --> Implement your solution here\n",
    "\n",
    "    return points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test the function with the following code for \n",
    "$$\n",
    "    \\mathbf{x}_0 = \n",
    "\\begin{pmatrix}\n",
    "    4\\\\\n",
    "    -1 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and play around with the optional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = torch.tensor([4.0, -1.0])\n",
    "path = incomplete_line_search(x_init, f)\n",
    "plot_contours(\n",
    "    x[..., 0], x[..., 1], f(x), opti=[-1, 1], paths={\"Incomplete line search\": path}\n",
    ")\n",
    "print(f\"Final values are x_1={path[-1][0]:.3f}, x_2={path[-1][1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Steepest decent method with complete line search\n",
    "\n",
    "We have a predefined function named `complete_line_search(x_init, func, maxiter=10)` that takes an initial point $\\mathbf{x}_0 \\in \\mathcal{R}^d$ named `x_init`, a function `func`, and an iteration limit `max_iter`.\n",
    "\n",
    "a) Implement a steepest gradient decent with completed line search re-using the previous `incomplete_line_search` to solve the subproblem of finding the optimal step size $\\eta^*_k$. The function should return a list of all steps $\\mathbf{x}_k \\in \\mathcal{R}^d$ taken during the optimization, i.e. `[[x1_0, x2_0, ..., xd_0], [x1_1, x2_1, ..., xd_1], ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_line_search(x_init, func, max_iter=10):\n",
    "    # Copy initial x to new differentiable tensor x\n",
    "    x = x_init.clone().requires_grad_()\n",
    "\n",
    "    points = []\n",
    "\n",
    "    # --> Implement your solution here\n",
    "\n",
    "    return points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test the function with the following code for \n",
    "$$\n",
    "    \\mathbf{x}_0 = \n",
    "\\begin{pmatrix}\n",
    "    4\\\\\n",
    "    -1 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and discuss why an incomplete line search is usually choosen over a complete line search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = torch.tensor([4.0, -1.0])\n",
    "path = complete_line_search(x_init, f)\n",
    "plot_contours(\n",
    "    x[..., 0], x[..., 1], f(x), opti=[-1, 1], paths={\"Complete line search\": path}\n",
    ")\n",
    "print(f\"Final values are x_1={path[-1][0]:.3f}, x_2={path[-1][1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Conjugated gradients\n",
    "We have a predefined function named `cg(x_init, func, maxiter=5)` that takes an initial point $\\mathbf{x}_0 \\in \\mathcal{R}^d$ named `x_init`, a function `func`, and an iteration limit `max_iter`.\n",
    "\n",
    "a) Implement the conjugated gradients method in that function re-using the previous `incomplete_line_search` to solve the subproblem of finding the optimal step size $\\eta^*_k$. The function should return a list of all steps $\\mathbf{x}_k \\in \\mathcal{R}^d$ taken during the optimization, i.e. `[[x1_0, x2_0, ..., xd_0], [x1_1, x2_1, ..., xd_1], ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg(x_init, func, max_iter=5):\n",
    "    # Copy initial x to new differentiable tensor x\n",
    "    x = x_init.clone().requires_grad_()\n",
    "\n",
    "    points = []\n",
    "\n",
    "    # --> Implement your solution here\n",
    "\n",
    "    return points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test the function with the following code for \n",
    "$$\n",
    "    \\mathbf{x}_0 = \n",
    "\\begin{pmatrix}\n",
    "    4\\\\\n",
    "    -1 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and discuss its benefits and drawbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = torch.tensor([4.0, -1.0])\n",
    "path = cg(x_init, f)\n",
    "plot_contours(\n",
    "    x[..., 0], x[..., 1], f(x), opti=[-1, 1], paths={\"Conjugated gradients\": path}\n",
    ")\n",
    "print(f\"Final values are x_1={path[-1][0]:.3f}, x_2={path[-1][1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - BFGS \n",
    "We have a predfined function named `bfgs(x_init, func, maxiter=5)` that takes an initial point $\\mathbf{x}_0 \\in \\mathcal{R}^d$ named `x_init`, a function `func`, and an iteration limit `max_iter`.\n",
    "\n",
    "a) Implement the BFGS method in that function re-using the previous `incomplete_line_search` to solve the subproblem of finding the optimal step size $\\eta^*_k$. The function should return a list of all steps $\\mathbf{x}_k \\in \\mathcal{R}^d$ taken during the optimization, i.e. `[[x1_0, x2_0, ..., xd_0], [x1_1, x2_1, ..., xd_1], ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(x_init, func, max_iter=5):\n",
    "    # Copy initial x to new differentiable tensor x\n",
    "    x = x_init.clone().requires_grad_()\n",
    "\n",
    "    points = []\n",
    "\n",
    "    # --> Implement your solution here\n",
    "\n",
    "    return points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test the function with the following code for \n",
    "$$\n",
    "    \\mathbf{x}_0 = \n",
    "\\begin{pmatrix}\n",
    "    4\\\\\n",
    "    -1 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and discuss its benefits and drawbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = torch.tensor([4.0, -1.0])\n",
    "path = bfgs(x_init, f)\n",
    "plot_contours(x[..., 0], x[..., 1], f(x), opti=[-1, 1], paths={\"BFGS\": path})\n",
    "print(f\"Final values are x_1={path[-1][0]:.3f}, x_2={path[-1][1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - Comparison\n",
    "The following code plots all optimization paths on the given quadratic problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = torch.tensor([4.0, -1.0])\n",
    "path_simple = simple_decent(x_init, f)\n",
    "path_ils = incomplete_line_search(x_init, f)\n",
    "path_cls = complete_line_search(x_init, f)\n",
    "path_cg = cg(x_init, f)\n",
    "path_bfgs = bfgs(x_init, f)\n",
    "plot_contours(\n",
    "    x[..., 0],\n",
    "    x[..., 1],\n",
    "    f(x),\n",
    "    opti=[-1, 1],\n",
    "    paths={\n",
    "        \"Simple\": path_simple,\n",
    "        \"ILS\": path_ils,\n",
    "        \"CLS\": path_cls,\n",
    "        \"CG\": path_cg,\n",
    "        \"BFGS\": path_bfgs,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic problem is a rather easy optimization problem. Compare the algorithms for some hard optimization test functions (`himmelblau_function` and `rosenbrock_function`) and different start points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def himmelblau_function(x):\n",
    "    return (x[..., 0] ** 2 + x[..., 1] - 11) ** 2 + (\n",
    "        x[..., 0] + x[..., 1] ** 2 - 7\n",
    "    ) ** 2\n",
    "\n",
    "\n",
    "# --> Implement your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define domain\n",
    "x0 = torch.linspace(-1.5, 1.5, steps=100)\n",
    "x1 = torch.linspace(-1.5, 1.5, steps=100)\n",
    "x = torch.stack(torch.meshgrid(x0, x1, indexing=\"xy\"), dim=2)\n",
    "\n",
    "\n",
    "def rosenbrock_function(x):\n",
    "    return 100 * (x[..., 1] - x[..., 0] ** 2) ** 2 + (1 - x[..., 0]) ** 2\n",
    "\n",
    "\n",
    "# --> Implement your solution here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
