{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04 - Local approximations\n",
    "\n",
    "The volume of a four-bar truss should be minimized under the displacement constraint $\\delta \\le \\delta_0$. There is a force $P>0$ acting along the direction of bar 3. All bars have identical lengths $l$ and identical Young's moduli $E$. The modifiable structural variables are the cross sectional areas $A_1=A_4$ and $A_2=A_3$. We define $A_0 = Pl / (10\\delta_0E)$ and constrain the variables $0.2A_0 \\le A_j \\le 2.5 A_0$. Then we can use dimensionless design variables $a_j=A_j/A_0 \\in [0.2, 2.5]$.\n",
    "\n",
    "\n",
    "![Four bar truss](../figures/four_bar_truss.png)\n",
    "\n",
    "\n",
    "Credits: Peter W. Christensen and Anders Klarbring. *An Introduction to Structural Optimization.* Springer Netherlands, 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import plot_contours\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Defining the constrained optimization problem\n",
    "\n",
    "a) Compute the objective function $f(\\mathbf{a})$ that should be minimized and define it as Python function that accepts inputs tensors of the shape [..., 2].\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define f(a)\n",
    "def f(a):\n",
    "    return a[..., 0] + a[..., 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compute the constraint function $g(\\mathbf{a})$ for $\\delta_0=0.1$ and define it as Python function that accepts inputs tensors of the shape [..., 2]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define g(a)\n",
    "def g(a):\n",
    "    first_term = 8/(16*a[...,0] + 9*a[...,1])\n",
    "    second_term = - 4.5/(9*a[...,0] + 16*a[...,1])\n",
    "    return  first_term +second_term - 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Summarize the optimization problem statement with all constraints. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - CONLIN\n",
    "\n",
    "a) Implement a function named `CONLIN(func, a_k)` that computes a CONLIN approximation of the function `func` at position `a_k`. `CONLIN` should return an approximation function that can be evaluated at any point `a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CONLIN(func, a_k):\n",
    "    # Implement your solution here\n",
    "    a_lin = a_k.clone().requires_grad_()\n",
    "    gradients = torch.autograd.grad(func(a_lin).sum(), a_lin)[0]\n",
    "\n",
    "    def approximation(a):\n",
    "        res = func(a_k)\n",
    "        for j, grad in enumerate(gradients):\n",
    "            if grad < 0.0:\n",
    "                Gamma = a_k[j] / a[..., j]\n",
    "            else:\n",
    "                Gamma = 1.0\n",
    "            res += grad * Gamma * (a[...,j] - a_k[j])\n",
    "        return res\n",
    "\n",
    "    return approximation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Solve the problem with sequential CONLIN approximations starting from $\\mathbf{a}^0 = (2,1)^\\top$ with the dual method. Record all intermediate points $\\mathbf{a}^0, \\mathbf{a}^1, \\mathbf{a}^2, ...$ in a list called `a` for later plotting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to compute minima and maxima in this procedure, hence you are given the `box_constrained_decent` method from the previous excercise to perform these operations. The method is slightly modified:\n",
    "- It takes extra arguments that can be passed to the function, e.g. by `box_constrained_decent(..., mu=1.0)` \n",
    "- It returns only the final result and not all intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_constrained_decent(\n",
    "    func, x_init, x_lower, x_upper, eta=0.1, max_iter=100, **extra_args\n",
    "):\n",
    "    x = x_init.clone().requires_grad_()\n",
    "    for _ in range(max_iter):\n",
    "        grad = torch.autograd.grad(func(x, **extra_args).sum(), x)[0]\n",
    "        x = x - eta * grad\n",
    "        x = torch.max(torch.min(x, x_upper), x_lower)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial values, lower bound, and upper bound of \"a\"\n",
    "a_0 = torch.tensor([2.0, 1.0])\n",
    "a_lower = torch.tensor([0.2, 0.2])\n",
    "a_upper = torch.tensor([2.5, 2.5])\n",
    "\n",
    "# Define the initial value, lower bound, and upper bound of \"mu\"\n",
    "mu_0 = torch.tensor([10.0])\n",
    "mu_lower = torch.tensor([0.0])\n",
    "mu_upper = torch.tensor([1000000000.0])\n",
    "\n",
    "# Define list of a \n",
    "a = [a_0]\n",
    "\n",
    "for k in range(5):\n",
    "    # Compute the current approximation function\n",
    "    g_tilde = CONLIN(g, a[k])\n",
    "\n",
    "    # Define the Lagrangian(a, mu)\n",
    "    def lagrangian(a, mu):\n",
    "        return f(a) + mu * g_tilde(a)\n",
    "\n",
    "    # Define a_star(mu) using `box_constrained_decent\n",
    "    # def a_star(mu):\n",
    "    #     return box_constrained_decent(lagrangian, a[k], a_lower, a_upper, mu=mu)\n",
    "    \n",
    "    # Define a_star by minimizing the Lagrangian w. r. t. a analytically\n",
    "    a_lin = a[k].clone().requires_grad_()\n",
    "    gradients = torch.autograd.grad(g(a_lin).sum(), a_lin)[0]\n",
    "\n",
    "    def a_star(mu):\n",
    "        a_hat = torch.zeros_like(gradients)\n",
    "        a_hat[gradients<0] = torch.sqrt(-mu*gradients[gradients<0]*a[k][gradients<0]**2)\n",
    "        a_hat[gradients>0] = a_lower[gradients>0]\n",
    "        return torch.max(torch.min(a_hat, a_upper), a_lower)\n",
    "\n",
    "    # Define the dual function \n",
    "    def dual_function(mu):\n",
    "        return -lagrangian(a_star(mu), mu)\n",
    "\n",
    "    # Compute the maximum of the dual function\n",
    "    mu_star = box_constrained_decent(dual_function, mu_0, mu_lower, mu_upper, eta=10.0)\n",
    "\n",
    "    # Plot the dual function (just for debuging)\n",
    "    # mu = torch.linspace(0.0, 20.0, 20)\n",
    "    # dual = [-dual_function(m) for m in mu]\n",
    "    # with torch.no_grad():\n",
    "    #     plt.plot(mu, dual)\n",
    "    #     plt.axvline(mu_star.item(), color=\"black\")\n",
    "    #     plt.title(f\"Dual function in iteration {k}\")\n",
    "    #     plt.show()\n",
    "\n",
    "    a.append(a_star(mu_star))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the results (works after solving the previous tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting domain\n",
    "a_1 = torch.linspace(0.1, 3.0, 200)\n",
    "a_2 = torch.linspace(0.1, 3.0, 200)\n",
    "aa = torch.stack(torch.meshgrid(a_1, a_2, indexing=\"xy\"), dim=2)\n",
    "\n",
    "# Make a plot\n",
    "plot_contours(\n",
    "    aa[..., 0],\n",
    "    aa[..., 1],\n",
    "    f(aa),\n",
    "    paths={\"CONLIN\": a},\n",
    "    box=[a_lower, a_upper],\n",
    "    opti=[a[-1][0], a[-1][1]],\n",
    ")\n",
    "plt.contour(a_1, a_2, g(aa), [0], colors=\"k\", linewidths=3)\n",
    "plt.contourf(a_1, a_2, g(aa), [0, 1], colors=\"gray\", alpha=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) This implementation is relatively slow. How could it be accelerated?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - MMA\n",
    "\n",
    "a) Implement a function named `MMA(func, a_k, L_k, U_k)` that computes a CONLIN approximation of the function `func` at position `a_k` with lower asymptotes `L_k` and uper asymtotes `U_k`. `MMA` should return an approximation function that can be evaluated at any point `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMA(func, a_k, L_k, U_k):\n",
    "    a_lin = a_k.clone().requires_grad_() \n",
    "    gradients = torch.autograd.grad(func(a_lin), a_lin)[0]\n",
    "\n",
    "    def approximation(a):\n",
    "        res = func(a_k)\n",
    "        for j, grad in enumerate(gradients):\n",
    "            if grad < 0.0:\n",
    "                p = 0\n",
    "                q = -(a_k[j]-L_k[j])**2 * grad\n",
    "            else:\n",
    "                p = (U_k[j] - a_k[j]) **2 * grad \n",
    "                q = 0\n",
    "            res -= p/(U_k[j]-a_k[j]) + q/(a_k[j]-L_k[j])\n",
    "            res += p/(U_k[j]-a[...,j]) + q/(a[...,j]-L_k[j])\n",
    "        return res\n",
    "\n",
    "    return approximation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Solve the problem with sequential MMA approximations starting from $\\mathbf{a}^0 = (2,1)^\\top$ with the dual method. Record all intermediate points $\\mathbf{a}^0, \\mathbf{a}^1, \\mathbf{a}^2, ...$ in a list called `a` for the asymptote updates and later plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial values, lower bound, and upper bound of \"a\"\n",
    "a_0 = torch.tensor([2.0, 1.0])\n",
    "a_lower = torch.tensor([0.2, 0.2])\n",
    "a_upper = torch.tensor([2.5, 2.5])\n",
    "\n",
    "# Define the initial value, lower bound, and upper bound of \"mu\"\n",
    "mu_0 = torch.tensor([10.0])\n",
    "mu_lower = torch.tensor([0.0])\n",
    "mu_upper = torch.tensor([1000000000.0])\n",
    "\n",
    "# Define lists for  a, L, and U \n",
    "a = [a_0]\n",
    "L = []\n",
    "U = []\n",
    "\n",
    "# Define factor s for shrinkage and growth of asymptotes\n",
    "s = 0.7\n",
    "\n",
    "for k in range(5):\n",
    "    # Update asymptotes with heuristic procedure\n",
    "    if k > 1:\n",
    "        L_k = torch.zeros_like(L[k - 1])\n",
    "        U_k = torch.zeros_like(U[k - 1])\n",
    "        for j in range(len(L_k)):\n",
    "            if (a[k][j] - a[k - 1][j]) * (a[k - 1][j] - a[k - 2][j]) < 0.0:\n",
    "                L_k[j] = a[k][j] - s * (a[k - 1][j] - L[k - 1][j])\n",
    "                U_k[j] = a[k][j] + s * (U[k - 1][j] - a[k - 1][j])\n",
    "            else:\n",
    "                L_k[j] = a[k][j] - 1.0 / sqrt(s) * (a[k - 1][j] - L[k - 1][j])\n",
    "                U_k[j] = a[k][j] + 1.0 / sqrt(s) * (U[k - 1][j] - a[k - 1][j])\n",
    "        L.append(L_k)\n",
    "        U.append(U_k)\n",
    "    else:\n",
    "        L.append(a[k] - s * (a_upper - a_lower))\n",
    "        U.append(a[k] + s * (a_upper - a_lower))\n",
    "\n",
    "    # Compute lower move limit in this step\n",
    "    a_lower_k = torch.maximum(a_lower, 0.9 * L[k] + 0.1 * a[k])\n",
    "    a_upper_k = torch.minimum(a_upper, 0.9 * U[k] + 0.1 * a[k])\n",
    "    \n",
    "    # Compute the current approximation function\n",
    "    g_tilde = MMA(g, a[k], L[k], U[k])\n",
    "    \n",
    "    # Define the Lagrangian(a, mu)\n",
    "    def lagrangian(a, mu):\n",
    "        return f(a) + mu * g_tilde(a)\n",
    "\n",
    "    # Define a_star(mu) using `box_constrained_decent\n",
    "    def a_star(mu):\n",
    "        return box_constrained_decent(lagrangian, a[k], a_lower, a_upper, mu=mu)\n",
    "\n",
    "    # Define the dual function \n",
    "    def dual_function(mu):\n",
    "        return -lagrangian(a_star(mu), mu)\n",
    "\n",
    "    # Compute the maximum of the dual function\n",
    "    mu_star = box_constrained_decent(dual_function, mu_0, mu_lower, mu_upper, eta=10.0)\n",
    "\n",
    "    a.append(a_star(mu_star))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the results (works after solving the previous tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting domain\n",
    "a_1 = torch.linspace(0.1, 3.0, 200)\n",
    "a_2 = torch.linspace(0.1, 3.0, 200)\n",
    "aa = torch.stack(torch.meshgrid(a_1, a_2, indexing=\"xy\"), dim=2)\n",
    "\n",
    "# Make a plot\n",
    "plot_contours(\n",
    "    aa[..., 0],\n",
    "    aa[..., 1],\n",
    "    f(aa),\n",
    "    paths={\"MMA\": a},\n",
    "    box=[a_lower, a_upper],\n",
    "    opti=[a[-1][0], a[-1][1]],\n",
    ")\n",
    "plt.contour(a_1, a_2, g(aa), [0], colors=\"k\", linewidths=3)\n",
    "plt.contourf(a_1, a_2, g(aa), [0, 1], colors=\"gray\", alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
